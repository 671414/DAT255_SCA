{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n"
   ],
   "id": "5dd41b85178240ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d44d12cb8b18987f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bOur goal is to create a dataset consisting of traces and attack points for model learning, and we will utilize plaintext and keys for this. We will use the stored information from data and key sets to do this. traces will function as X and attack points will be Y. In this notebook we are testing the data. The test dataset will be gathered on the same chipwhisperer, but with a new key each time?",
   "id": "fa0416626ea2591e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Right now, for this version i only gather 65636 traces, but i also collect the sub_bytes_out from the trace, this seems to take up a lot more time then previous, when we only gathered text, key and trace.",
   "id": "d67d9027110dd3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Loading 256*500 for testin, usefull to not use up all my memory\n",
    "# Load the h5py file\n",
    "file_path = \"C:\\\\Users\\\\Kaspar\\\\ChipWhisperer5_64\\\\cw\\\\home\\\\portable\\\\chipwhisperer\\\\jupyter\\\\courses\\\\sca101\\\\TINYAES_test_set.hdf5\"\n",
    "with h5py.File(file_path, \"r\") as h5_file:\n",
    "    plaintext_set = h5_file['data'][:65536]\n",
    "    trace_set = h5_file['trace'][:65536]\n",
    "    key_set = h5_file['key'][:65536]\n",
    "    #sub_bytes_out_set = h5_file['sub_bytes_out'][:65536]\n"
   ],
   "id": "5728722aa89f7ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(trace_set[0][:100])"
   ],
   "id": "f6d7f363f2ef9852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create a dataset consisting og plaintext and keys so that we can pre-compute sub_bytes_in, which is one of the attack points. This is done by XOR on key and plaintext",
   "id": "cf15773a40e32dd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = tf.data.Dataset.from_tensor_slices((plaintext_set_set, key_set))\n",
   "id": "155e84607efc118"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also have to pre compute our attack points, as we use CW firmware we do not have the SBOX available at the moment, and will therefore rely more on sub_bytes_in\n",
   "id": "7e9f179a2b8b6e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##Kan lage en loop over denne for å beregne alle attack points, men det får bli senere\n",
    "\n",
    "\n",
    "# Define dataset variables for attack data\n",
    "attack_byte = []\n",
    "\n",
    "# Calculate attack points\n",
    "for key, data in dataset:\n",
    "    # Compute relevant attack point, assuming sub_byte_in as an example\n",
    "    # Here, we're aiming to simulate an attack point based on specific indexing logic\n",
    "    # Assuming sub_byte_index refers to the index of the targeted byte\n",
    "    sub_byte_index = 0  # Example: targeting the first byte\n",
    "    sub_byte_in = data[sub_byte_index] ^ key[sub_byte_index]\n",
    "\n",
    "    # Store computed attack point\n",
    "    attack_byte.append(sub_byte_in)\n",
    "\n",
    "#sørger for at punktene er på riktig format\n",
    "#attack_byte = to_categorical(attack_points, 256)\n",
    "# Convert attack points to a TensorFlow tensor for compatibility with training\n",
    "attack_byte = tf.convert_to_tensor(attack_byte, dtype=tf.uint8)\n",
    "\n"
   ],
   "id": "18f8032ff1bce55b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Making sure that the matrix is of the form byte_id, example_id",
   "id": "ebc86a0191a88d82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "attack_byte.shape",
   "id": "be6fbf85cca6f880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "key_set.shape",
   "id": "f1a70fc4993b59bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trace_set.shape",
   "id": "4dc7635eec96d2ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_set.shape",
   "id": "83fa9c89ad4866b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the data is stored the \"wrong\" way, we want byteid, example so we must transpose it in the process",
   "id": "c8cb41de1c90d802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(attack_byte[0])",
   "id": "c82a61f4b3fcfcdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adding attack points to the dataset, in this case this is for index 0",
   "id": "23ac7ad5cdbba55c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = tf.data.Dataset.from_tensor_slices((data_set, trace_set, key_set, attack_byte))",
   "id": "d33d85e02896ea37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At the moment i can fit everything in memory, when using the whole dataset the shuffle buffer will be 128 000, because of us using 500 traces per key, this should be enough. Batch size might vary",
   "id": "d7f4e0fd615bafae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature scaling on the power trace, so all values will be between -1 and 1(scaaml advice for making models converge",
   "id": "560efc7c1e02ca26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trace_set = tf.keras.layers.Rescaling(1./127.5, offset=-1)(trace_set)",
   "id": "55bf09a799ad8d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(trace_set[0])",
   "id": "eb088a81706e5bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trace_set = tf.expand_dims(trace_set, axis=-1)",
   "id": "b539f3e9a42c265a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Making sure that the values are scaled properly, and adding another dimension to the trace. This makes sure that it is compatible with Conv1D layers",
   "id": "542091fd4853cb9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(trace_set[10])",
   "id": "6adab7d287bbac51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b6f33c375da1bdd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8fcd11e175ce817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Shard seems to have a 3x16 matrix with keys, subin and subout, and then the bytes\n",
    "it loops through and gets the attack byte of the specified point from all the shards as y_shard and something else for x_shard. So i must save all the shards, and then get them. seems like the dataset only ends up with 256 points though?\n",
    "\n",
    "500 traces per key, 500*16 attack bytes and 3 attack points. but when loading a shard it fetches shard[attack_point][attack_byte], and then y =y[:num_traces:per_shard] which will be 500, so it gets them all for this byte?"
   ],
   "id": "a4fa2c46db3fc321"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a831ec7a83c43bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "290688586bc5e0fc"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
